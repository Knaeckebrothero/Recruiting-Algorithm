import os
import glob
import torch
# import torch.nn.utils.rnn as rnn
from transformers import AutoModelForCausalLM
import time
import warnings


def generate_output(input_tokens, generation_model) -> torch.Tensor:
    """
    This function is used to generate the model output given the input tokens.

    :param input_tokens: The input tokens from the token file.
    :param generation_model: The model used to generate the output.
    :return: The output tokens generated by the model.
    """
    # Ensure tokens are on the correct device
    gpu_tokens = input_tokens.to(generation_model.device)
    
    # Ignore no attention mask mimimi stuff
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Generate the output
        output = generation_model.generate(
            input_ids=gpu_tokens,
            eos_token_id=[128009, 128009],
            do_sample=True,
            temperature=0.3,
        )

    # Return the result starting from just after the input sequence ends
    return output[:, gpu_tokens.size(1):]


def process_files(input_dir, output_dir, model, batch_size=1) -> tuple[float, float]:
    """
    This function is used to process all files in the input directory
    and save them to the output to the output directory.
    It also calculates the average GPU utilization and memory usage for benchmarking purposes.

    :param input_dir: The directory containing the input files.
    :param output_dir: The directory to save the output files to.
    :param model: The model used to generate the output (must be on the final device).
    :param batch_size: The number of files to process in each batch (currently trows an error if not 1).

    :return: The average GPU utilization and memory usage.
    """
    # Ensure the output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # List all .pt files in the input directory
    file_paths = glob.glob(os.path.join(input_dir, '*.pt'))
    
    # Load all tensors into memory
    tensors = [torch.load(file_path) for file_path in file_paths]
    
    # Initialize metrics
    total_gpu_utilization = 0
    total_gpu_memory = 0
    num_measurements = 0
    
    # Batch process the tensors
    for i in range(0, len(tensors), batch_size):
        batch_tensors = tensors[i:i+batch_size]
        
        # Concatenate the tensors along the first dimension
        # batch_input_tokens = torch.stack(batch_tensors).to(model.device)
        batch_input_tokens = torch.cat(batch_tensors, dim=0).to(model.device)
        
        # Generate output from the model
        batch_response = generate_output(batch_input_tokens, model)
        
        # Save the output tensors to the output directory
        for j, response in enumerate(batch_response):
            basename = os.path.basename(file_paths[i+j])
            output_file_path = os.path.join(output_dir, basename)
            torch.save(response.cpu(), output_file_path)
        
        # Print progress
        print(f"Processed {i+len(batch_tensors)} files")
        
        # Record GPU utilization and memory usage every 5 generations
        if (i + len(batch_tensors)) % 5 == 0:
            gpu_utilization = torch.cuda.utilization()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # Convert bytes to MB
            total_gpu_utilization += gpu_utilization
            total_gpu_memory += gpu_memory
            num_measurements += 1
            print(f"GPU Utilization: {gpu_utilization}%, GPU Memory: {gpu_memory} MB")
    
    # Calculate average GPU utilization and memory usage
    avg_gpu_utilization = total_gpu_utilization / num_measurements
    avg_gpu_memory = total_gpu_memory / num_measurements

    # Return the metrics
    return avg_gpu_utilization, avg_gpu_memory


# Check if GPU available and set device to cuda if so
if not torch.cuda.is_available():
    print('CUDA not available...')
else:
    # Set device and print script start
    device = torch.device("cuda")
    print('CUDA available, running benchmark...')
    
    # Load model in 16bit mode and auto device mapping
    model = AutoModelForCausalLM.from_pretrained(
        # Requires huggingface login!
        # You might wanna start a notebook and login there before running this script
        # Use: from huggingface_hub import notebook_login
        # For: notebook_login()
        "meta-llama/Meta-Llama-3-8B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    # Print device model
    print(f'Model has been loaded to the following device: {model.device}')
    
    # Start timer
    start_time = time.time()
    
    # Process all files
    avg_gpu_utilization, avg_gpu_memory = process_files(
        'inputs',
        'outputs',
        model
    )
    
    # End timer and calculate elapsed time in minutes
    end_time = time.time()
    elapsed_time = (end_time - start_time) / 60
    
    # Print time and metrics
    print(f"Total processing time: {elapsed_time:.2f} minutes")
    print(f"Average GPU Utilization: {avg_gpu_utilization:.2f}%")
    print(f"Average GPU Memory: {avg_gpu_memory:.2f} MB")
    
    # Save metrics to a file
    metrics_file = 'metrics.txt'
    with open(metrics_file, 'w') as file:
        file.write(f"Total processing time: {elapsed_time:.2f} minutes\n")
        file.write(f"Average GPU Utilization: {avg_gpu_utilization:.2f}%\n")
        file.write(f"Average GPU Memory: {avg_gpu_memory:.2f} MB\n")
