{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# MongoDBs connection string\n",
    "mongo_url = os.getenv(\"MongoClientURI\")\n",
    "\n",
    "# Define the collection name\n",
    "mongo_collection_name = \"KGL_LIN_PRF_USA\"\n",
    "\n",
    "# Connect to the MongoDB database\n",
    "mongodb = MongoClient(mongo_url)[\"raw_data\"]\n",
    "collection = mongodb[mongo_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_experience(experience_dict: dict):\n",
    "    \"\"\"\n",
    "    Preprocess a experience from a LinkedIn profile by cleaning text fields.\n",
    "    \n",
    "    :param experience_dict: A dictionary containing profile information.\n",
    "\n",
    "    :return: A preprocessed dictionary with cleaned text fields.\n",
    "    \"\"\"\n",
    "    # Define the keys to extract and clean\n",
    "    keys_to_extract = ['company', 'title', 'description', 'location']\n",
    "    \n",
    "    # Initialize an empty dictionary to store the preprocessed data\n",
    "    preprocessed_data = {}\n",
    "    \n",
    "    # Iterate through the required keys\n",
    "    for key in keys_to_extract:\n",
    "        # Extract the value from the profile dictionary or use an empty string if the key is not present\n",
    "        value = experience_dict.get(key, '')\n",
    "        \n",
    "        # Clean up the text:\n",
    "        # - Strip leading and trailing whitespace\n",
    "        # - Replace multiple spaces with a single space\n",
    "        # - Replace new line characters and tabs with a single space\n",
    "        if value:\n",
    "            cleaned_value = re.sub(r'\\s+', ' ', value.strip())\n",
    "        else:\n",
    "            cleaned_value = None\n",
    "        \n",
    "        # Store the cleaned value in the preprocessed data dictionary\n",
    "        preprocessed_data[key] = cleaned_value\n",
    "    \n",
    "    # Return the preprocessed data\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'Limited Brands', 'title': 'Raw Materials Intern- MAST Global', 'description': '- Supported the day to day management of the print and pattern process - Worked cross functionally with Design, Tech Design, and Pre-production - Sat in on sketch line review and created PowerPoints to be sent out to our regional partners in order to receive costing - Communicated with our regional partners to follow up on lab-dips, strike-offs, and sampling as requested by the Design team - Completed numerous projects that were presented to co-workers for eye opening opportunities to further progress the growth of the brand', 'location': 'Columbus, Ohio Area'}\n"
     ]
    }
   ],
   "source": [
    "test_experience = {\n",
    "      \"starts_at\": {\n",
    "        \"day\": 1,\n",
    "        \"month\": 5,\n",
    "        \"year\": 2012\n",
    "      },\n",
    "      \"ends_at\": {\n",
    "        \"day\": 31,\n",
    "        \"month\": 8,\n",
    "        \"year\": 2012\n",
    "      },\n",
    "      \"company\": \"Limited Brands\",\n",
    "      \"company_linkedin_profile_url\": \"https://www.linkedin.com/company/lbrands\",\n",
    "      \"title\": \"Raw Materials Intern- MAST Global\",\n",
    "      \"description\": \"- Supported the day to day management of the print and pattern process\\n- Worked cross functionally with Design, Tech Design, and Pre-production\\n- Sat in on sketch line review and created PowerPoints to be sent out to our regional partners in order to receive costing\\n- Communicated with our regional partners to follow up on lab-dips, strike-offs, and sampling as requested by the Design team\\n- Completed numerous projects that were presented to co-workers for eye opening opportunities to further progress the growth of the brand\",\n",
    "      \"location\": \"Columbus, Ohio Area\",\n",
    "      \"logo_url\": \"https://media-exp1.licdn.com/dms/image/C4E0BAQHD8okj9rA0EQ/company-logo_100_100/0/1547228096275?e=1655942400&v=beta&t=2y-txs4X8PWRyl-UFfX_lGv0JtryM8MA7AQqNN6wlqo\"\n",
    "    }\n",
    "\n",
    "result = process_experience(test_experience)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def prepare_model_input(preprocessed_dict: dict, tokenizer: AutoTokenizer):\n",
    "    # Convert the dictionary to a JSON string to maintain structure in the prompt\n",
    "    data_json = json.dumps(preprocessed_dict, indent=None)\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"Extract and categorize information from the following LinkedIn profile entry: {data_json}\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  30059,    323,  22824,    553,   2038,    505,    279,   2768,\n",
      "          33867,   5643,   4441,     25,   5324,  10348,    794,    330,  75577,\n",
      "          55332,    498,    330,   2150,    794,    330,  20613,  32009,   4514,\n",
      "             12,    386,   6483,   8121,    498,    330,   4789,    794,   6660,\n",
      "          50080,    279,   1938,    311,   1938,   6373,    315,    279,   1194,\n",
      "            323,   5497,   1920,    482,   5664,    291,   5425,    734,    750,\n",
      "            449,   7127,     11,  17829,   7127,     11,    323,   5075,  70666,\n",
      "            482,  13479,    304,    389,  26610,   1584,   3477,    323,   3549,\n",
      "           7572,  11665,    311,    387,   3288,    704,    311,   1057,  15481,\n",
      "           8717,    304,   2015,    311,   5371,  54824,    482,  16838,    660,\n",
      "            449,   1057,  15481,   8717,    311,   1833,    709,    389,  10278,\n",
      "           1773,   3153,     11,  13471,  65039,     11,    323,  25936,    439,\n",
      "          11472,    555,    279,   7127,   2128,    482,  46594,  12387,   7224,\n",
      "            430,   1051,  10666,    311,   1080,  63384,    369,   8071,   8736,\n",
      "          10708,    311,   4726,   5208,    279,   6650,    315,    279,   6883,\n",
      "            498,    330,   2588,    794,    330,     34,   1152,  10551,     11,\n",
      "          14689,  12299,   9388]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = os.getenv(\"LLAMA_3_PATH\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Preprocess the experience\n",
    "result = process_experience(test_experience)\n",
    "\n",
    "# Get tokenized input ready for the model\n",
    "tokenized_input = prepare_model_input(result, tokenizer)\n",
    "\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = [\n",
    "    {\"name\": \"KGL_LIN_PRF_USA\", \"id_origin\": 1},\n",
    "    {\"name\": \"KGL_LIN_PRF_IND\", \"id_origin\": 2},\n",
    "    {\"name\": \"KGL_LIN_PRF_CAN\", \"id_origin\": 3},\n",
    "    {\"name\": \"KGL_LIN_PRF_SNG\", \"id_origin\": 4},\n",
    "    {\"name\": \"KGL_LIN_PRF_ISR\", \"id_origin\": 5},\n",
    "    {\"name\": \"KGL_LIN_PRF_BRS\", \"id_origin\": 6},\n",
    "    {\"name\": \"KGL_LIN_PRF_JPN\", \"id_origin\": 7},\n",
    "    {\"name\": \"KGL_LIN_PRF_DEN\", \"id_origin\": 8}\n",
    "]\n",
    "\n",
    "\n",
    "# Run the insertion function\n",
    "insert_collection_documents(\n",
    "    mongo_collection_name,\n",
    "    dwh_id_origin,\n",
    "    mysql_url,\n",
    "    mongo_url,\n",
    "    dwh_schema_name\n",
    ")\n",
    "\n",
    "    # Skip if  without experiences\n",
    "    if not doc.get('experiences', []):\n",
    "        continue\n",
    "\n",
    "    # Initialize counter variables\n",
    "    counter = 0\n",
    "    total = collection.count_documents({})\n",
    "\n",
    "    # Get the collection cursor\n",
    "    documents = collection.find()\n",
    "\n",
    "    # Processing loop\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            # Print progress\n",
    "            counter += 1\n",
    "            print(f\"{collection_str}: {counter}/{total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
