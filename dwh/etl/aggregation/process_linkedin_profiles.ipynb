{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52496751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example experience from a LinkedIn profile\n",
    "experience = {\n",
    "    \"company\": \"Old Dominion University\",\n",
    "    \"company_linkedin_profile_url\": \"https://www.linkedin.com/company/old-dominion-university\",\n",
    "    \"title\": \"Teaching Assistant\",\n",
    "    \"description\": \"Key responsibilities: \\n\\n1. Assisting students in executing experimental laboratory works for Electronic Circuit Analysis. \\n2. Administering exams, grading scripts and conducting problem solving sessions for Circuit Analysis I, Linear System Analysis, Electronic Circuit\",\n",
    "    \"location\": \"Norfolk, VA, USA.\",\n",
    "    \"logo_url\": \"https://media-exp1.licdn.com/dms/image/C4E0BAQG5Y7rGx9ZScQ/company-logo_100_100/0/1532523823768?e=1655942400&v=beta&t=sx3tGOZ4ApDAt8nOhp6purIOxuUNRyD1MP31VF-B0rA\"\n",
    "  }\n",
    "\n",
    "# Example education from a LinkedIn profile\n",
    "education = {\n",
    "    \"field_of_study\": \"Electrical and Computer Engineering\",\n",
    "    \"degree_name\": \"Doctor of Philosophy - PhD\",\n",
    "    \"school\": \"Old Dominion University\",\n",
    "    \"school_linkedin_profile_url\": \"https://www.linkedin.com/school/old-dominion-university/\",\n",
    "    \"description\": \"PhD Dissertation topic:\\nInterface engineering of highly efficient organic-inorganic hybrid perovskite solar cells.\",\n",
    "    \"logo_url\": \"https://media-exp1.licdn.com/dms/image/C4E0BAQG5Y7rGx9ZScQ/company-logo_100_100/0/1532523823768?e=1655942400&v=beta&t=sx3tGOZ4ApDAt8nOhp6purIOxuUNRyD1MP31VF-B0rA\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a23ed1",
   "metadata": {},
   "source": [
    "## GPT\n",
    "In addition to open source models i suggest trying comercial models as well.\n",
    "All thought we might also wanna consider anthropic's stuff, we might have to limit this test to openai due to time constraints.\n",
    "\n",
    "Create an account and save your api key to the OPENAI_API_KEY environment variable.\n",
    "https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424f4fce87d573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def openai_tag(dictionary_to_tag: dict, openai_client):\n",
    "    # Set prompt\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that specializes in generating test data in JSON format. The user will provide you with a JSON structure and you generate a complete example JSON with realistic sounding values.\"}, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": str(dictionary_to_tag)\n",
    "        }]\n",
    "\n",
    "    # Generate a completion\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Get the generated text\n",
    "    result_string = completion.choices[0].message.content\n",
    "    \n",
    "    # Convert the generated text into a dictionary\n",
    "    tagged_dict = json.loads(result_string)\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return tagged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ce994668d6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "openai_api = OpenAI()\n",
    "\n",
    "# Test tagging the education dictionary\n",
    "result = openai_tag(education, openai_api)\n",
    "print(result)\n",
    "\n",
    "# Test tagging the experience dictionary\n",
    "result = openai_tag(experience, openai_api)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3c995",
   "metadata": {},
   "source": [
    "## CUDA support\n",
    "Install the following software to run the code with CUDA support:\n",
    "\n",
    "1. C++: https://visualstudio.microsoft.com/downloads/\n",
    "2. CUDA toolkit: https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local\n",
    "3. Torch CUDA: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bad525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d96dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def check_memory_usage(model_path: str, bits: int = 32) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the memory usage of a model based on its number of parameters and the specified precision.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): The path or identifier of the model to be loaded.\n",
    "    - bits (int): The number of bits per parameter. Common values are 16, 32, or 64 bits.\n",
    "\n",
    "    Returns:\n",
    "    - float: The estimated memory size of the model in megabytes (MB).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model \n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # Calculate the model size in MB\n",
    "    num_parameters = loaded_model.num_parameters()\n",
    "\n",
    "    # Calculate memory usage based on the number of bits per parameter\n",
    "    model_size_MB = num_parameters * (bits / 8) / (1024 ** 2)\n",
    "\n",
    "    # Free up memory again\n",
    "    del loaded_model\n",
    "    gc.collect()\n",
    "\n",
    "    return model_size_MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b39533",
   "metadata": {},
   "source": [
    "## Mistral\n",
    "Let's try out the TextBase-7B-v0.1 model from the SF-Foundation.\n",
    "It is a instruction model based on the mistral and was selected because \n",
    "it scored highest on the hf leaderboard with a hella swag of 90.\n",
    "\n",
    "Clone repo and save path to the TEXT_BASE_PATH environment variable.\n",
    "https://huggingface.co/SF-Foundation/TextBase-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eeb362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52242ddf4e32471ca79690a98b979763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 13812.51 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Check size \n",
    "size_in_mb = check_memory_usage(model_path=os.getenv(\"TEXT_BASE_PATH\"), bits=16)\n",
    "\n",
    "print(f\"Model size: {size_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc971996842e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import transformers\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Load the model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(os.getenv(\"TEXT_BASE_PATH\"))\n",
    "\n",
    "# Print the model size\n",
    "print(f\"Model size: {model.get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80988068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"SF-Foundation/TextBase-7B-v0.1\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SF-Foundation/TextBase-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"SF-Foundation/TextBase-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3ff184d972bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T17:20:01.188771Z",
     "start_time": "2024-06-08T17:19:45.257336Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def generate_output(input_text):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/TextBase-7B-v0.1\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"/TextBase-7B-v0.1\")\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,  # Adjust the maximum length of the generated output as needed\n",
    "        num_return_sequences=1,  # Number of output sequences to generate\n",
    "        temperature=0.7,  # Adjust the temperature for controlling the randomness of the output\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a16f00a108c24c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T17:22:07.019836Z",
     "start_time": "2024-06-08T17:20:04.185280Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"Here is an example LinkedIn profile entry:\\n...\"\n",
    "output = generate_output(input_text)\n",
    "print(\"Generated output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31297c494c1c14a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T18:11:04.704598Z",
     "start_time": "2024-06-09T18:11:01.569630Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "model_path = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "def generate_output(input_text):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def generate_output_stream(input_text):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the output token by token\n",
    "    for output in model.generate(\n",
    "        input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        num_beams=1,\n",
    "    ):\n",
    "        # Decode the generated token\n",
    "        token = tokenizer.decode(output.sequences[:, -1], skip_special_tokens=True)\n",
    "\n",
    "        # Yield the decoded token\n",
    "        yield token\n",
    "\n",
    "    # Finalize the generated text\n",
    "    generated_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "    yield generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011d517d3909718",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a test, if the model is working correctly.\"\n",
    "output = generate_output(input_text)\n",
    "print(\"Generated output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b113a9a7f5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Here is an example LinkedIn profile entry:\\n...\"\n",
    "for token in generate_output(input_text):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    # Process the partial output if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b110e4",
   "metadata": {},
   "source": [
    "## Llama 3 \n",
    "I just wanned to try the new Llama3 (using 8b instruction variant).\n",
    "To use it you have to create a meta account to request access.\n",
    "\n",
    "Clone repo and save path to the LLAMA_3_PATH environment variable.\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6ece86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505ab3133cc341c6be2eed99ed709c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 15316.51 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Check size \n",
    "size_in_mb = check_memory_usage(model_path=os.getenv(\"LLAMA_3_PATH\"), bits=16)\n",
    "\n",
    "print(f\"Model size: {size_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1aa318b0fa4e71d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T18:37:15.756311Z",
     "start_time": "2024-06-09T18:35:04.120304Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import transformers\n",
    "\n",
    "\n",
    "def llama_tag(dictionary_to_tag: dict, llm_pipeline: transformers.pipeline) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes a dictionary and tags it using the specified LLM pipeline.\n",
    "\n",
    "    :param dictionary_to_tag: The dictionary to tag.\n",
    "    :param llm_pipeline: The LLM pipeline to use for tagging.\n",
    "    :return: The tagged dictionary (If an error occurs, the message is returned under the key error in the dict).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that specializes in generating test data in JSON format. The user will provide you with a JSON structure and you generate a complete example JSON with realistic sounding values.\"},\n",
    "            {\"role\": \"user\", \"content\": str(dictionary_to_tag)}]\n",
    "\n",
    "        \n",
    "        # Define terminators\n",
    "        terminators = [\n",
    "        llm_pipeline.tokenizer.eos_token_id,\n",
    "        llm_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # Generate a completion\n",
    "        outputs = llm_pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "            # eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        # Get the generated text\n",
    "        result_string = outputs[0][\"generated_text\"][-1]\n",
    "        \n",
    "        # Convert the generated text into a dictionary\n",
    "        tagged_dict = json.loads(result_string)\n",
    "\n",
    "        # Return the dictionary\n",
    "        return tagged_dict\n",
    "    \n",
    "    # Handle exceptions\n",
    "    except transformers.errors.ModelOutputError as e:\n",
    "        return {\"error\": str(e)}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to decode JSON\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set up the Llama model as a pipeline\n",
    "llama_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=os.getenv(\"LLAMA_3_PATH\"),\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "# Test tagging the education dictionary\n",
    "result = llama_tag(education, llama_pipeline)\n",
    "print(result)\n",
    "\n",
    "# Test tagging the experience dictionary\n",
    "result = llama_tag(experience, llama_pipeline)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231146212b0075b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "model_id = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Tokenize the input messages\n",
    "input_ids = tokenizer.encode(messages[0][\"content\"], return_tensors=\"pt\").to(device)\n",
    "for message in messages[1:]:\n",
    "    message_ids = tokenizer.encode(message[\"content\"], return_tensors=\"pt\").to(device)\n",
    "    input_ids = torch.cat([input_ids, message_ids], dim=-1)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Generate the output token by token\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=input_ids.shape[1] + 256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "\n",
    "# Stream the generated output\n",
    "for i in range(input_ids.shape[1], output_ids.sequences.shape[1]):\n",
    "    token = tokenizer.decode(output_ids.sequences[0, i], skip_special_tokens=True)\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "# Print the final generated text\n",
    "generated_text = tokenizer.decode(output_ids.sequences[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6b7485a2c2be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T18:30:01.045188Z",
     "start_time": "2024-06-09T18:30:01.022152Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Model size: {model.get_memory_usage():.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
