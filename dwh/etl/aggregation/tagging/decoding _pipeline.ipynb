{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T10:35:03.975260Z",
     "start_time": "2024-06-16T10:35:03.969623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def postprocess_and_save_experiences(input_folder_path, model_id):\n",
    "    \"\"\"\n",
    "    This Function decodes the outputs.\n",
    "    \n",
    "    :param input_folder_path: The path to the folder containing the .pt files.\n",
    "    :param model_id: The model id for the tokenizer.\n",
    "    \"\"\"\n",
    "    # List all .pt files in the input directory\n",
    "    file_paths = glob.glob(os.path.join(input_folder_path, '*.pt'))\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Initialize counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Load all tensors into memory\n",
    "    for file_path in file_paths:\n",
    "        tensor = torch.load(file_path)\n",
    "        \n",
    "        # Decode the tensor\n",
    "        decoded_tensor = model_tokenizer.batch_decode(tensor, skip_special_tokens=True)\n",
    "        \n",
    "        # Get the file name without the extension\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        # Create the output file path with .json extension\n",
    "        output_file_path = os.path.join(input_folder_path, file_name + '.json')\n",
    "        \n",
    "        # Save decoded result as JSON\n",
    "        with open(output_file_path, 'w') as json_file:\n",
    "            json.dump(decoded_tensor, json_file)\n",
    "        \n",
    "        # Up and print profile counter\n",
    "        counter += 1\n",
    "        print(counter)"
   ],
   "id": "b69fff46ac97a53f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Run the decoding function\n",
    "postprocess_and_save_experiences(os.getenv(\"TAGS_FOLDER_PATH\")+'outputs', os.getenv(\"LLAMA_3_PATH\"))"
   ],
   "id": "51e6d353ac2172c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T10:58:56.117311Z",
     "start_time": "2024-06-16T10:58:55.656379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def postprocess_and_save_experiences_full(prompt_folder_path, response_folder_path, model_id):\n",
    "    \"\"\"\n",
    "    :param prompt_folder_path: The path to the folder containing the prompt .pt files.\n",
    "    :param response_folder_path: The path to the folder containing the response .pt files.\n",
    "    :param model_id: The model name of which tokenizer to use for decoding the files.\n",
    "    \"\"\"\n",
    "    # List all .pt files in the prompt directory\n",
    "    prompt_file_paths = glob.glob(os.path.join(prompt_folder_path, '*.pt'))\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Process each prompt file\n",
    "    for prompt_file_path in prompt_file_paths:\n",
    "        # Get the file name without the extension\n",
    "        file_name = os.path.splitext(os.path.basename(prompt_file_path))[0]\n",
    "        \n",
    "        # Load the prompt tensor\n",
    "        prompt_tensor = torch.load(prompt_file_path)\n",
    "        \n",
    "        # Decode the prompt tensor\n",
    "        decoded_prompt = model_tokenizer.batch_decode(prompt_tensor, skip_special_tokens=True)\n",
    "        \n",
    "        # Find the corresponding response file\n",
    "        response_file_path = os.path.join(response_folder_path, file_name + '.pt')\n",
    "        \n",
    "        # Check if the response file exists\n",
    "        if os.path.exists(response_file_path):\n",
    "            # Load the response tensor\n",
    "            response_tensor = torch.load(response_file_path)\n",
    "            \n",
    "            # Decode the response tensor\n",
    "            decoded_response = model_tokenizer.batch_decode(response_tensor, skip_special_tokens=True)\n",
    "            \n",
    "            # Create a dictionary to store the prompt and response\n",
    "            experience = {\n",
    "                'prompt': decoded_prompt,\n",
    "                'response': decoded_response\n",
    "            }\n",
    "            \n",
    "            # Create the output file path with .json extension\n",
    "            output_file_path = os.path.join(response_folder_path, file_name + '.json')\n",
    "            \n",
    "            # Save the prompt and response as JSON\n",
    "            with open(output_file_path, 'w') as json_file:\n",
    "                json.dump(experience, json_file)\n",
    "        else:\n",
    "            print(f\"Response file not found for prompt: {file_name}\")"
   ],
   "id": "27d9c21ab45329c3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T10:59:04.428669Z",
     "start_time": "2024-06-16T10:58:59.068955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Run the decoding function\n",
    "postprocess_and_save_experiences_full(\n",
    "    os.path.join(os.getenv(\"TAGS_FOLDER_PATH\"), 'inputs/'), \n",
    "    os.path.join(os.getenv(\"TAGS_FOLDER_PATH\"), 'outputs/'),\n",
    "    os.getenv(\"LLAMA_3_PATH\")\n",
    ")"
   ],
   "id": "4ecbc6defb8707ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
