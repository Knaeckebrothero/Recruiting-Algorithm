{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:51:17.414527Z",
     "start_time": "2024-06-17T17:51:13.378719Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def process_experience(experience_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Function to preprocess an experience from a LinkedIn profile by cleaning text fields.\n",
    "    The function extracts the company, title, description, and location fields from the input dictionary.\n",
    "    It then cleans up the text by removing extra whitespace and new line characters.\n",
    "    \n",
    "    :param experience_dict: A dictionary containing profile information.\n",
    "\n",
    "    :return: A preprocessed dictionary with cleaned text fields.\n",
    "    \"\"\"\n",
    "    # Define the keys to extract and clean\n",
    "    keys_to_extract = ['company', 'title', 'description', 'location']\n",
    "    \n",
    "    # Initialize an empty dictionary to store the preprocessed data\n",
    "    preprocessed_data = {}\n",
    "    \n",
    "    # Iterate through the required keys\n",
    "    for key in keys_to_extract:\n",
    "        # Extract the value from the profile dictionary or use an empty string if the key is not present\n",
    "        value = experience_dict.get(key, '')\n",
    "        \n",
    "        # Clean up text\n",
    "        if value:\n",
    "            cleaned_value = re.sub(r'\\s+', ' ', value.strip())\n",
    "        else:\n",
    "            cleaned_value = None\n",
    "        \n",
    "        # Store the cleaned value in the preprocessed data dictionary\n",
    "        preprocessed_data[key] = cleaned_value\n",
    "    \n",
    "    # Return the preprocessed data\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "def prepare_experience_for_model_input(preprocessed_dict: dict, system_prompt: str, tokenizer: AutoTokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Function to prepare the model input based on the preprocessed dictionary.\n",
    "    The function constructs a prompt message using the system prompt and the preprocessed dictionary.\n",
    "    It then tokenizes the prompt using the provided tokenizer and returns the input IDs for the model.\n",
    "\n",
    "    :param preprocessed_dict: The preprocessed experience as a dictionary.\n",
    "    :param system_prompt: The prompt to use for the model input.\n",
    "    :param tokenizer: The tokenizer to use for encoding the input.\n",
    "\n",
    "    :return: The input IDs for the model (aka the tokens).\n",
    "    \"\"\"\n",
    "    # Convert the preprocessed dictionary to a JSON string and construct the prompt\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(preprocessed_dict, indent=None)},\n",
    "    ]\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Return the input IDs\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def preprocess_and_save_experiences(input_file_path, output_path, system_prompt: str, model_id_or_path: str):\n",
    "    \"\"\"\n",
    "    Function to preprocess a JSON file containing the LinkedIn profiles and save the processed data to a new file.\n",
    "    The function reads the input JSON file, preprocesses the experiences, and prepares the model input.\n",
    "    It then saves the processed data to a new JSON file (object id + list of encoded experiences).\n",
    "    \n",
    "    :param input_file_path: The path to the input JSON file.\n",
    "    :param output_path: The path to save the output JSON file.\n",
    "    :param system_prompt: The prompt to use for the model input.\n",
    "    :param model_id_or_path: The identifier or path for the model to use for tokenization.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the processed results\n",
    "    total_results = []\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_or_path)\n",
    "    \n",
    "    # Open and load the JSON file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # Initialize counter\n",
    "    profile_counter = 0\n",
    "\n",
    "    # Iterate over each profile in the data\n",
    "    for profile in data:\n",
    "        # Initialize a counter for every profile\n",
    "        experience_list_counter = 1\n",
    "        \n",
    "        # Iterate over each experience in the profile\n",
    "        for experience in profile[\"experiences\"]:            \n",
    "            # Preprocess the experience\n",
    "            preprocessed_experience = process_experience(experience)\n",
    "            \n",
    "            # Prepare the experience for the model input\n",
    "            encoded_experience = prepare_experience_for_model_input(\n",
    "                preprocessed_dict=preprocessed_experience, \n",
    "                system_prompt=system_prompt,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            \n",
    "            # Serialize each experience individually and save it to a file\n",
    "            torch.save(encoded_experience, f'{output_path}/input_{profile[\"_id\"][\"$oid\"]}_{experience_list_counter}.pt')\n",
    "            \n",
    "            # Up the counter\n",
    "            experience_list_counter += 1\n",
    "        \n",
    "        # Up and print profile counter\n",
    "        profile_counter += 1\n",
    "        print(profile_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85881f361b655874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:41:34.399892Z",
     "start_time": "2024-06-15T11:41:34.382728Z"
    }
   },
   "outputs": [],
   "source": [
    "test_experiences = [{\n",
    "      \"company\": \"University of Engineering and Technology Peshawar, Pakistan\",\n",
    "      \"title\": \"R&D Consultant\",\n",
    "      \"description\": \"UET, Peshawar has undertaken various projects that required the expertise of a R&D based Biomedical Engineer. I provided my expertise to the students and staff in these projects and increased the productivity of the department. \\n\\nAdditionally, I\\n• Identified the need for 3D printers for the department\\n• Investigated 3D printers that would be best suited for the training individuals\\n• Designed a certified course to train the students and staff on the essentials of 3D printing\\n• Trained individuals including but not limited to, students, professionals and artists\",\n",
    "      \"location\": \"Pakistan\"\n",
    "    },\n",
    "    {\n",
    "      \"company\": \"Google\",\n",
    "      \"title\": \"Software Engineering Intern\",\n",
    "      \"description\": None,\n",
    "      \"location\": \"Venice, California\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for experience in test_experiences:\n",
    "    print(\"Before processing:\")\n",
    "    print(experience)\n",
    "    print(\"\\nAfter processing:\")\n",
    "    print(process_experience(experience))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae0c886bf2ee5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:41:37.456929Z",
     "start_time": "2024-06-15T11:41:37.224221Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "    \n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.getenv(\"LLAMA_3_PATH\"))\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "for experience in test_experiences:\n",
    "    preprocessed_experience = process_experience(experience)\n",
    "    tokenized_input = prepare_experience_for_model_input(\n",
    "        process_experience(experience), sys_prompt, tokenizer)\n",
    "    \n",
    "    print(\"Encoded input:\")\n",
    "    print(tokenized_input)\n",
    "    \n",
    "    print(\"\\nDecoded input:\")\n",
    "    print(tokenizer.decode(tokenized_input[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2758156a03d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:55:45.095435Z",
     "start_time": "2024-06-17T17:55:05.825330Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "    \n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# Run the preparation function\n",
    "preprocess_and_save_experiences(\n",
    "    input_file_path=os.getenv(\"INPUT_FILE_PATH\"),\n",
    "    output_path= os.getenv(\"OUTPUT_PATH\"),\n",
    "    system_prompt=sys_prompt,\n",
    "    model_id_or_path='microsoft/Phi-3-mini-128k-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc1f7b",
   "metadata": {},
   "source": [
    "# Phi-3 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def preprocess_object(\n",
    "        object_dict: dict, \n",
    "        keys_to_extract: list = ['company', 'title', 'description', 'location']\n",
    "        ) -> dict:\n",
    "    \"\"\"\n",
    "    This function preprocesses a dictionary containing profile information by cleaning text fields.\n",
    "    \n",
    "    :param object_dict: A dictionary containing profile information.\n",
    "    :param keys_to_extract: A list of keys to extract and clean from the input dictionary.\n",
    "\n",
    "    :return: A preprocessed dictionary with cleaned text fields.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary to store the preprocessed data\n",
    "    preprocessed_data = {}\n",
    "    \n",
    "    # Iterate through the required keys\n",
    "    for key in keys_to_extract:\n",
    "        # Extract the value from the profile dictionary or use an empty string if the key is not present\n",
    "        value = object_dict.get(key, '')\n",
    "        \n",
    "        # Clean up text\n",
    "        if value:\n",
    "            cleaned_value = re.sub(r'\\s+', ' ', value.strip())\n",
    "        else:\n",
    "            cleaned_value = None\n",
    "        \n",
    "        # Store the cleaned value in the preprocessed data dictionary\n",
    "        preprocessed_data[key] = cleaned_value\n",
    "    \n",
    "    # Return the preprocessed data\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "def format_prompt(input_dict: dict, system_prompt: str, model_tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Function to format the input dictionary as a prompt message for the model.\n",
    "\n",
    "    :param input_dict: The input dictionary to format.\n",
    "    :param system_prompt: The system prompt to use for the model input.\n",
    "\n",
    "    :return: The formatted prompt message as a single string.\n",
    "    \"\"\"\n",
    "    # Formate and return the inputs as a single string\n",
    "    return model_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(input_dict, indent=None)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def encode_prompt(string_prompt: str, tokenizer: AutoTokenizer, seed: int = random.randint(0, 1000)):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(string_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Return the input IDs\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Set the output path\n",
    "output_path = os.getenv(\"OUTPUT_PATH\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(os.getenv(\"INPUT_FILE_PATH\"), 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Find the length of the longest prompt\n",
    "max_prompt_length = 0\n",
    "\n",
    "for profile in data:\n",
    "    for experience in profile[\"experiences\"]:\n",
    "        preprocessed_experience = preprocess_object(experience, keys_to_extract=['company', 'title', 'description'])\n",
    "        prompt_length = len(format_prompt(preprocessed_experience, system_prompt=sys_prompt, model_tokenizer=tokenizer))\n",
    "        max_prompt_length = max(max_prompt_length, prompt_length)\n",
    "\n",
    "# Initialize counter\n",
    "profile_counter = 0\n",
    "\n",
    "# Iterate over each profile in the data\n",
    "for profile in data:\n",
    "    experience_list_counter = 1\n",
    "    \n",
    "    for experience in profile[\"experiences\"]:\n",
    "        preprocessed_experience = preprocess_object(experience, keys_to_extract=['company', 'title', 'description'])\n",
    "        formatted_experience_prompt = format_prompt(preprocessed_experience, system_prompt=sys_prompt, model_tokenizer=tokenizer)\n",
    "        \n",
    "        # Encode the prompt with padding\n",
    "        encoded_prompt = tokenizer(\n",
    "            formatted_experience_prompt,\n",
    "            #padding='max_length',\n",
    "            #max_length=max_prompt_length,\n",
    "            truncation=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoded_experience = {\n",
    "            'input_ids': encoded_prompt['input_ids'][0],\n",
    "            'attention_mask': encoded_prompt['attention_mask'][0]\n",
    "        }\n",
    "        print(encoded_experience)\n",
    "        \n",
    "        torch.save(encoded_experience, f'{output_path}/input_{profile[\"_id\"][\"$oid\"]}_{experience_list_counter}.pt')\n",
    "        \n",
    "        experience_list_counter += 1\n",
    "    \n",
    "    profile_counter += 1\n",
    "    print(profile_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "# Set the paths to the token files\n",
    "file_path_1 = \"\"\n",
    "file_path_2 = \"\"\n",
    "\n",
    "# Load the token files\n",
    "encoded_experience_1 = torch.load(file_path_1)\n",
    "encoded_experience_2 = torch.load(file_path_2)\n",
    "\n",
    "# Move the model and input tensors to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "input_ids_1 = encoded_experience_1['input_ids'].to(device)\n",
    "input_ids_2 = encoded_experience_2['input_ids'].to(device)\n",
    "\n",
    "# Generate output for the first token file\n",
    "start_time_1 = time.time()\n",
    "with torch.no_grad():\n",
    "    output_1 = model.generate(input_ids=input_ids_1, max_new_tokens=128)\n",
    "end_time_1 = time.time()\n",
    "\n",
    "# Generate output for the second token file\n",
    "start_time_2 = time.time()\n",
    "with torch.no_grad():\n",
    "    output_2 = model.generate(input_ids=input_ids_2, max_new_tokens=128)\n",
    "end_time_2 = time.time()\n",
    "\n",
    "# Calculate the time difference for each generation\n",
    "time_diff_1 = end_time_1 - start_time_1\n",
    "time_diff_2 = end_time_2 - start_time_2\n",
    "\n",
    "print(f\"Generation time for token file 1: {time_diff_1:.4f} seconds\")\n",
    "print(f\"Generation time for token file 2: {time_diff_2:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "file_path = \"input_6656faee44160735d73fa8df_3.pt\"\n",
    "\n",
    "# Load the saved file\n",
    "loaded_data = torch.load(file_path)\n",
    "\n",
    "# Check if the loaded data contains the attention mask\n",
    "if 'attention_mask' in loaded_data:\n",
    "    print(\"Attention mask is present in the loaded data.\")\n",
    "    attention_mask = loaded_data['attention_mask']\n",
    "    print(\"Attention mask shape:\", attention_mask.shape)\n",
    "else:\n",
    "    print(\"Attention mask is not present in the loaded data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fe1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Set the output path\n",
    "output_path = os.getenv(\"OUTPUT_PATH\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(os.getenv(\"INPUT_FILE_PATH\"), 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counter\n",
    "profile_counter = 0\n",
    "\n",
    "# Collect all the formatted prompts\n",
    "formatted_prompts = []\n",
    "\n",
    "# Iterate over each profile in the data\n",
    "for profile in data:\n",
    "    for experience in profile[\"experiences\"]:\n",
    "        preprocessed_experience = preprocess_object(experience, keys_to_extract=['company', 'title', 'description', 'location'])\n",
    "        formatted_experience_prompt = format_prompt(preprocessed_experience, system_prompt=sys_prompt, model_tokenizer=tokenizer)\n",
    "        formatted_prompts.append(formatted_experience_prompt)\n",
    "\n",
    "# Encode the prompts with padding\n",
    "encoded_prompts = tokenizer(formatted_prompts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Iterate over each profile in the data again\n",
    "for profile in data:\n",
    "    experience_list_counter = 1\n",
    "    \n",
    "    for experience in profile[\"experiences\"]:\n",
    "        encoded_tokens = encoded_prompts['input_ids'][profile_counter]\n",
    "        attention_mask = encoded_prompts['attention_mask'][profile_counter]\n",
    "        \n",
    "        encoded_experience = {\n",
    "            'input_ids': encoded_tokens,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        \n",
    "        torch.save(encoded_experience, f'{output_path}/input_{profile[\"_id\"][\"$oid\"]}_{experience_list_counter}.pt')\n",
    "        \n",
    "        experience_list_counter += 1\n",
    "    \n",
    "    profile_counter += 1\n",
    "    print(profile_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Set the output path\n",
    "output_path = os.getenv(\"OUTPUT_PATH\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(os.getenv(\"INPUT_FILE_PATH\"), 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Initialize counter\n",
    "profile_counter = 0\n",
    "\n",
    "# Iterate over each profile in the data\n",
    "for profile in data:\n",
    "    # Initialize a counter for every profile\n",
    "    experience_list_counter = 1\n",
    "    \n",
    "    # Iterate over each experience in the profile\n",
    "    for experience in profile[\"experiences\"]:\n",
    "        # Preprocess the experience\n",
    "        preprocessed_experience = preprocess_object(\n",
    "            experience, \n",
    "            keys_to_extract=['company', 'title', 'description', 'location']\n",
    "            )\n",
    "        \n",
    "        # Format the experience as a prompt\n",
    "        formatted_experience_prompt = format_prompt(\n",
    "            preprocessed_experience,\n",
    "            system_prompt=sys_prompt,\n",
    "            model_tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "        # Encode the prompt\n",
    "        encoded_experience = encode_prompt(\n",
    "            formatted_experience_prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            seed=1\n",
    "            )\n",
    "        \n",
    "        # Serialize each experience individually and save it to a file\n",
    "        torch.save(encoded_experience, f'{output_path}/input_{profile[\"_id\"][\"$oid\"]}_{experience_list_counter}.pt')\n",
    "        \n",
    "        # Up the counter\n",
    "        experience_list_counter += 1\n",
    "    \n",
    "    # Up and print profile counter\n",
    "    profile_counter += 1\n",
    "    print(profile_counter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and pad\n",
    "encoding = tokenizer(texts, \n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=128, \n",
    "                    add_special_tokens=True\n",
    "                    )\n",
    "\n",
    "# Move to device\n",
    "inputs = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "tokens = tokenizer.encode(\"How are you today? Can you say, fine?\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=device, \n",
    "    torch_dtype=\"auto\"\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ddad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate(\n",
    "    tokens.to(device), \n",
    "    max_new_tokens=256,\n",
    "    do_sample=False\n",
    "    )\n",
    "\n",
    "print(result)\n",
    "\n",
    "output = tokenizer.decode(result[0], skip_special_tokens= True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19007d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your are a python developer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Help me generate a bubble algorithm\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 600,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.3,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820829fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ") # trust_remote_code=False\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "test_experiences = [{\n",
    "      \"company\": \"University of Engineering and Technology Peshawar, Pakistan\",\n",
    "      \"title\": \"R&D Consultant\",\n",
    "      \"description\": \"UET, Peshawar has undertaken various projects that required the expertise of a R&D based Biomedical Engineer. I provided my expertise to the students and staff in these projects and increased the productivity of the department. \\n\\nAdditionally, I\\n• Identified the need for 3D printers for the department\\n• Investigated 3D printers that would be best suited for the training individuals\\n• Designed a certified course to train the students and staff on the essentials of 3D printing\\n• Trained individuals including but not limited to, students, professionals and artists\",\n",
    "      \"location\": \"Pakistan\"\n",
    "    },\n",
    "    {\n",
    "      \"company\": \"Google\",\n",
    "      \"title\": \"Software Engineering Intern\",\n",
    "      \"description\": None,\n",
    "      \"location\": \"Venice, California\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "\n",
    "\"\"\"\n",
    "for experience in test_experiences:\n",
    "    print(\"Before processing:\")\n",
    "    print(experience)\n",
    "    print(\"\\nAfter processing:\")\n",
    "    print(process_experience(experience))\n",
    "\"\"\"\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "test = format_prompt(test_experiences[0], sys_prompt, tokenizer)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "tokens = tokenizer.tokenize(test, \n",
    "                            return_tensors=\"pt\", \n",
    "                            add_special_tokens=True)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "output = model.generate(tokens)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def format_prompt(input_dict: dict, system_prompt: str, model_tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Function to format the input dictionary as a prompt message for the model.\n",
    "\n",
    "    :param input_dict: The input dictionary to format.\n",
    "    :param system_prompt: The system prompt to use for the model input.\n",
    "\n",
    "    :return: The formatted prompt message as a single string.\n",
    "    \"\"\"\n",
    "    # Formate and return the inputs as a single string\n",
    "    return model_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(input_dict, indent=None)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=device, \n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Example texts\n",
    "texts = [\"How are you today?\", \"Can you say, fine?\", \"Hello there! How's everything?\"]\n",
    "\n",
    "# Tokenize and pad\n",
    "encoding = tokenizer(texts, \n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=128, \n",
    "                    add_special_tokens=True\n",
    "                    )\n",
    "\n",
    "print(encoding)\n",
    "\n",
    "# Move to device\n",
    "inputs = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "outputs = model.generate(\n",
    "    inputs, \n",
    "    attention_mask=attention_mask,\n",
    "    max_length=256,\n",
    "    max_new_tokens=50,  # Adjust based on your needs\n",
    "    do_sample=False  # Or True if you want varied responses\n",
    ")\n",
    "\n",
    "# Decode the outputs\n",
    "decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Print each response\n",
    "for response in decoded_outputs:\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
