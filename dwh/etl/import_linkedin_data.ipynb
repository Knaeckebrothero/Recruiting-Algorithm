{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "hostname = os.getenv('DATABASE_HOST')\n",
    "user = os.getenv('DATABASE_USER')\n",
    "password = os.getenv('DATABASE_PASSWORD')\n",
    "database = os.getenv('DATABASE_NAME') \n",
    "\n",
    "# Set up database connection using SSL\n",
    "db_engine = create_engine(f'mysql+pymysql://{user}:{password}@{hostname}/{database}', \n",
    "                       connect_args={\n",
    "                           'ssl_ca': os.getenv('SSL_CA'),\n",
    "                           'ssl_cert': os.getenv('SSL_CLIENT_CERT'),\n",
    "                           'ssl_key': os.getenv('SSL_KEY')\n",
    "                       })\n",
    "\n",
    "\"\"\"\n",
    "!!!UNENCRYPTED CONNECTION!!!\n",
    "For use within local network only!\n",
    "db_engine = create_engine(f'mysql+pymysql://{user}:{password}@{hostname}/{database}') \n",
    "!!!UNENCRYPTED CONNECTION!!!\n",
    "\"\"\""
   ],
   "id": "582a09ee191de892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to check data types and max lengths of values\n",
    "def check_data(dataset):\n",
    "    for column in dataset.columns:\n",
    "        print(f'Column: {column}')\n",
    "        print(f'Data type: {dataset[column].dtype}')\n",
    "        try:\n",
    "            print(f'Max length: {dataset[column].str.len().max()}')\n",
    "        except:\n",
    "            print('No string data ')\n",
    "        print('---\\n')"
   ],
   "id": "5459649c948f61f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = os.getenv('DATASET_PATH')\n",
    "\n",
    "# Read JSONs from file\n",
    "df = pd.read_json(path, lines=True)\n",
    "\n",
    "check_data(df)"
   ],
   "id": "c20a63cfc0f32810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to check if language already exists\n",
    "def check_and_insert_language(languages_data_entry, db_con):\n",
    "    # Define query to find existing record\n",
    "    query = \"\"\"\n",
    "        SELECT id\n",
    "        FROM DIM_Languages\n",
    "        WHERE listOfLanguages = %(listOfLanguages)s\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query database for existing record\n",
    "    query_result = pd.read_sql_query(query, db_con, params=languages_data_entry)\n",
    "\n",
    "    # Check if a record was found or insert a new record\n",
    "    if not query_result.empty:\n",
    "        # Record found -> use its id\n",
    "        entry_id = query_result.iloc[0]['id']\n",
    "    else:\n",
    "        # No record found -> insert new record and query its id\n",
    "        languages_df = pd.DataFrame([languages_data_entry])\n",
    "        languages_df.to_sql('DIM_Languages', con=db_con, if_exists='append', index=False)\n",
    "        entry_id = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_con).iloc[0, 0]\n",
    "    \n",
    "    # Return ID\n",
    "    return entry_id\n",
    "\n",
    "# Function to check if location already exists\n",
    "def check_and_insert_location(location_data_entry, db_con):\n",
    "    # Define query to find existing record\n",
    "    query = \"\"\"\n",
    "        SELECT id\n",
    "        FROM DIM_Location\n",
    "        WHERE countryName = %(countryName)s\n",
    "        AND stateName = %(stateName)s\n",
    "        AND cityName = %(cityName)s\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query database for existing record\n",
    "    query_result = pd.read_sql_query(query, db_con, params=location_data_entry)\n",
    "\n",
    "    # Check if a record was found or insert a new record\n",
    "    if not query_result.empty:\n",
    "        # Record found -> use its id\n",
    "        entry_id = query_result.iloc[0]['id']\n",
    "    else:\n",
    "        # No record found -> insert new record and query its id\n",
    "        location_df = pd.DataFrame([location_data_entry])\n",
    "        location_df.to_sql('DIM_Location', con=db_con, if_exists='append', index=False)\n",
    "        entry_id = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_con).iloc[0, 0]\n",
    "    \n",
    "    # Return ID\n",
    "    return entry_id"
   ],
   "id": "cdd0780c0f8dd4d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to insert data origin and return its ID\n",
    "def insert_data_origin(data_origin_name, data_origin_url, data_origin_comment):\n",
    "    df_origin = pd.DataFrame([{\n",
    "        'name': data_origin_name, \n",
    "        'url': data_origin_url, \n",
    "        'importDate': pd.Timestamp.now(), \n",
    "        'comment': data_origin_comment}])\n",
    "\n",
    "    # Insert data into database\n",
    "    df_origin.to_sql('DIM_DataOrigin', con=db_engine, if_exists='append', index=False)\n",
    "    \n",
    "    # Get ID of inserted data origin\n",
    "    id_origin = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_engine).iloc[0, 0]\n",
    "    \n",
    "    # Return ID of inserted data origin\n",
    "    return id_origin"
   ],
   "id": "75b7d320581b5edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "272c7f1b6c833266",
   "metadata": {},
   "source": [
    "# Function to process person facts + dimensions and insert them into the database\n",
    "def process_json(json_object, dimension_key_origin, db_con):\n",
    "    \"\"\"\n",
    "    Extract data from LinkedIn profiles JSON and prepare it for insertion into DWH.\n",
    "\n",
    "    Missing attributes will be added as comments, \n",
    "    complex attributes will be receiving their own tables and be set to id None for now.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data for Person table\n",
    "    person_data = {\n",
    "        'idOrigin': dimension_key_origin,\n",
    "        'linkedInPubId': json_object.get('public_identifier'),\n",
    "        'profilePictureUrl': json_object.get('profile_pic_url'),\n",
    "        'backgroundPictureUrl': json_object.get('background_cover_image_url'),\n",
    "        # Last name\n",
    "        # First name\n",
    "        'name': json_object.get('full_name'),\n",
    "        'occupation': json_object.get('occupation'),\n",
    "        'profileHeadline': json_object.get('headline'),\n",
    "        'profileText': json_object.get('summary'),\n",
    "        'idLocation': None,\n",
    "        # Experiences\n",
    "        # Education\n",
    "        'idLanguages': None,\n",
    "        # Accomplishments (organisations, publications, honor awards, patents, courses, projects, test scores)\n",
    "        # Volunteer work\n",
    "        # Certifications\n",
    "        # Connections (included but no need to rename)\n",
    "        # Activities\n",
    "        # Similar named profiles\n",
    "        # Articles\n",
    "        # Groups\n",
    "        # Skills\n",
    "        # Infrared salary\n",
    "        # Github (included but no need to rename)\n",
    "        # Facebook (included but no need to rename)\n",
    "        # Gender (will be converted to enum M/F)\n",
    "        'birthDate': json_object.get('birth_date'),\n",
    "        # Industry\n",
    "        # Interests\n",
    "    }\n",
    "\n",
    "    # Extract data for Location table\n",
    "    location_data = {\n",
    "        'countryName': json_object.get('country_full_name'),\n",
    "        'countryLetters': json_object.get('country'),\n",
    "        'stateName': json_object.get('state'),\n",
    "        'cityName': json_object.get('city')\n",
    "    }\n",
    "    \n",
    "    # Extract data for Languages table\n",
    "    languages_data = {\n",
    "        'sumOfSpoken': len(json_object.get('languages', [])),\n",
    "        'listOfLanguages': ', '.join(json_object.get('languages', []))\n",
    "    }\n",
    "    \n",
    "    # Create DataFrames\n",
    "    person_df = pd.DataFrame([person_data])\n",
    "    languages_df = pd.DataFrame([languages_data])\n",
    "    location_df = pd.DataFrame([location_data])\n",
    "    \n",
    "    # Insert languages and add ID to person\n",
    "    languages_df.to_sql('DIM_Languages', con=db_con, if_exists='append', index=False)\n",
    "    dimension_key_languages = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_con).iloc[0, 0]\n",
    "    person_df['idLanguages'] = dimension_key_languages\n",
    "    \n",
    "    # Insert location and add ID to person\n",
    "    location_df.to_sql('DIM_Location', con=db_con, if_exists='append', index=False)\n",
    "    dimension_key_location = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_con).iloc[0, 0]\n",
    "    person_df['idLocation'] = dimension_key_location\n",
    "    \n",
    "    # Insert person data and get ID\n",
    "    person_df.to_sql('FACT_Person', con=db_con, if_exists='append', index=False)\n",
    "    pk_person = pd.read_sql_query(\"SELECT LAST_INSERT_ID()\", db_con).iloc[0, 0]\n",
    "    \n",
    "    # Return primary key of inserted person\n",
    "    return pk_person"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    \n",
    "    # Extract data for Qualification table\n",
    "    qualification_data = []\n",
    "    for qualification in json_object.get('qualifications', []):\n",
    "        qualification_data.append({\n",
    "            'idOrigin': json_object.get('idOrigin'),\n",
    "            'idPerson': None,  # Placeholder for idPerson\n",
    "            'idDuration': None,  # Placeholder for idDuration\n",
    "            'type': qualification.get('type'),\n",
    "            'name': qualification.get('name'),\n",
    "            'idInstitution': None,  # Placeholder for idInstitution\n",
    "            'description': qualification.get('description')\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    qualification_df = pd.DataFrame(qualification_data)\n",
    "\n",
    "    # Insert data into the database\n",
    "    engine = create_engine('your_database_url')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Insert data into FACT_Qualification table\n",
    "    qualification_df.to_sql('FACT_Qualification', con=engine, if_exists='append', index=False)"
   ],
   "id": "65c5b338154dc045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "# Function to get schema of a MongoDB collection and return it as a dictionary\n",
    "def get_schema(collection):\n",
    "    collection_schema = {}\n",
    "    total_documents = collection.count_documents({})\n",
    "\n",
    "    # Iterate over the collection\n",
    "    for document in collection.find():\n",
    "        for key, value in document.items():\n",
    "            # Add attribute to schema if not present already\n",
    "            if key not in collection_schema:\n",
    "                collection_schema[key] = {\n",
    "                    \"types\": set(),\n",
    "                    \"max_length\": 0,\n",
    "                    # Maybe include median length or 90% quartile?\n",
    "                    \"exists\": 0,\n",
    "                    \"null_percentage\": 0,\n",
    "                    \"max_value\": None\n",
    "                }\n",
    "            \n",
    "            # Determine type\n",
    "            collection_schema[key][\"types\"].add(type(value).__name__)\n",
    "            \n",
    "\n",
    "            # Calculate size and check if value is null\n",
    "            if isinstance(value, str):\n",
    "                length = len(value)\n",
    "                if length > collection_schema[key][\"max_length\"]:\n",
    "                    collection_schema[key][\"max_length\"] = length\n",
    "                    collection_schema[key][\"max_value\"] = value\n",
    "                \n",
    "                # Update null counter\n",
    "                collection_schema[key][\"exists\"] += 1\n",
    "            elif isinstance(value, (int, float)):\n",
    "                # Skip for numbers\n",
    "                # if collection_schema[key][\"max_value\"] is None or value > collection_schema[key][\"max_value\"]:\n",
    "                #    collection_schema[key][\"max_value\"] = value\n",
    "                \n",
    "                # Update null counter\n",
    "                collection_schema[key][\"exists\"] += 1\n",
    "            elif isinstance(value, list):\n",
    "                length = len(value)\n",
    "                if length > collection_schema[key][\"max_length\"]:\n",
    "                    collection_schema[key][\"max_length\"] = length\n",
    "                \n",
    "                # Update null counter\n",
    "                collection_schema[key][\"exists\"] += 1\n",
    "            elif value is not None:\n",
    "                # Update null counter\n",
    "                collection_schema[key][\"exists\"] += 1\n",
    "            elif value is None:\n",
    "                # Update null counter with null value\n",
    "                collection_schema[key][\"exists\"] += 1\n",
    "                collection_schema[key][\"null_percentage\"] += 1\n",
    "            else:\n",
    "                # Idk why i did this... Me no brain xD\n",
    "                print(value[\"_id\"])\n",
    "                print(type(value).__name__)\n",
    "    \n",
    "    # Calculate the population\n",
    "    for key in collection_schema:\n",
    "        value_populated = collection_schema[key][\"exists\"]\n",
    "        collection_schema[key][\"exists\"] = value_populated / total_documents * 100\n",
    "        collection_schema[key][\"null_percentage\"] =\\\n",
    "            (100 - (collection_schema[key][\"null_percentage\"] / value_populated * 100))\n",
    "\n",
    "    return collection_schema\n",
    "\n",
    "\n",
    "# Function to print schema dictionary\n",
    "def print_schema(schema_dict):\n",
    "    for key, value in schema_dict.items():\n",
    "        print(f\"Field: {key}\")\n",
    "        print(f\"  Types: {', '.join(value['types'])}\")\n",
    "        print(f\"  Max Length: {value['max_length']}\")\n",
    "        print(f\"  Exists on: {value['exists']:.2f}%\")\n",
    "        print(f\"  Population: {value['null_percentage']:.2f}%\")\n",
    "        print(f\"  Max Value: {value['max_value']}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Function to get schema of a MongoDB collection and return it as a dictionary\n",
    "def get_nested_schema(collection, schema_dict):\n",
    "    nested_schema = {}\n",
    "\n",
    "    for key, value in schema_dict.items():\n",
    "        if 'list' in value['types']:\n",
    "            nested_schema[key] = {}\n",
    "\n",
    "            total_documents = 0\n",
    "            for document in collection.find():\n",
    "                if key in document and isinstance(document[key], list):\n",
    "                    total_documents += 1\n",
    "                    for nested_doc in document[key]:\n",
    "                        if isinstance(nested_doc, dict):\n",
    "                            for nested_key, nested_value in nested_doc.items():\n",
    "                                if nested_key not in nested_schema[key]:\n",
    "                                    nested_schema[key][nested_key] = {\n",
    "                                        \"types\": set(),\n",
    "                                        \"max_length\": 0,\n",
    "                                        \"exists\": 0,\n",
    "                                        \"null_percentage\": 0,\n",
    "                                        \"max_value\": None\n",
    "                                    }\n",
    "\n",
    "                                nested_schema[key][nested_key][\"types\"].add(type(nested_value).__name__)\n",
    "\n",
    "                                if isinstance(nested_value, str):\n",
    "                                    length = len(nested_value)\n",
    "                                    if length > nested_schema[key][nested_key][\"max_length\"]:\n",
    "                                        nested_schema[key][nested_key][\"max_length\"] = length\n",
    "                                        nested_schema[key][nested_key][\"max_value\"] = nested_value\n",
    "\n",
    "                                    nested_schema[key][nested_key][\"exists\"] += 1\n",
    "                                elif isinstance(nested_value, (int, float)):\n",
    "                                    nested_schema[key][nested_key][\"exists\"] += 1\n",
    "                                elif nested_value is not None:\n",
    "                                    nested_schema[key][nested_key][\"exists\"] += 1\n",
    "                                elif nested_value is None:\n",
    "                                    nested_schema[key][nested_key][\"null_percentage\"] += 1\n",
    "\n",
    "            for nested_key, nested_value in nested_schema[key].items():\n",
    "                value_populated = nested_value[\"exists\"]\n",
    "                nested_value[\"exists\"] = value_populated / total_documents * 100\n",
    "                if value_populated > 0:\n",
    "                    nested_value[\"null_percentage\"] = (\n",
    "                        100 - (nested_value[\"null_percentage\"] / value_populated * 100)\n",
    "                    )\n",
    "                else:\n",
    "                    nested_value[\"null_percentage\"] = 0\n",
    "\n",
    "    return nested_schema\n",
    "\n",
    "\n",
    "# Function to print the nested schema\n",
    "def print_nested_schema(schema_dict):\n",
    "    for key, value in schema_dict.items():\n",
    "        print(f\"Nested Field: {key}\")\n",
    "        for nested_key, nested_value in value.items():\n",
    "            if nested_key != \"types\":\n",
    "                print(f\"    Attribute: {nested_key}\")\n",
    "                print(f\"         Types: {', '.join(nested_value['types'])}\")\n",
    "                print(f\"         Max Length: {nested_value['max_length']}\")\n",
    "                print(f\"         Exists on: {nested_value['exists']:.2f}%\")\n",
    "                print(f\"         Population: {nested_value['null_percentage']:.2f}%\")\n",
    "                print(f\"         Max Value: {nested_value['max_value']}\")\n",
    "                print()"
   ],
   "id": "488f49cea4a61834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection details\n",
    "client = MongoClient(os.getenv(\"MongoClientURI\"))\n",
    "db = client[\"dwh_sources\"]\n",
    "\n",
    "# Build the schema\n",
    "schema = get_schema(db[\"kaggle_linkedin_proxycurl_profiles\"])\n",
    "\n",
    "# Print the schema\n",
    "print_schema(schema)"
   ],
   "id": "2b34b050449d9ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the schema\n",
    "schema_nested = get_nested_schema(db[\"kaggle_linkedin_proxycurl_profiles\"], schema)\n",
    "\n",
    "# Print the schema\n",
    "print_nested_schema(schema_nested)"
   ],
   "id": "b34d6b4d2c05ea0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
