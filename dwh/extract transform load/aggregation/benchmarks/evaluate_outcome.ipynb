{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3190b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "openai_api = OpenAI()\n",
    "\n",
    "# Directory containing all files to process\n",
    "results_directory = os.getenv('TAGS_FOLDER_PATH') + 'outputs_json/'\n",
    "\n",
    "# Iterate over all files in the results directory\n",
    "for filename in os.listdir(results_directory):\n",
    "    if filename.endswith(\".json\"):  # Adjust the file type if necessary\n",
    "        file_path = os.path.join(results_directory, filename)\n",
    "        # Evaluate the result for each file\n",
    "        \n",
    "json.dumps(open(generation_result_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:07:24.176560Z",
     "start_time": "2024-06-16T12:07:24.170839Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def evaluate_result(generation_result_path: str, system_prompt: str, openai_client):\n",
    "    # Construct prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(open(generation_result_path).read())}\n",
    "    ]\n",
    "\n",
    "    # Generate a completion\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        # model=\"gpt-4-turbo\",\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Get the generated text\n",
    "    result_string = completion.choices[0].message.content\n",
    "\n",
    "    # Convert the generated text into a dictionary\n",
    "    evaluation = json.loads(result_string)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee423338b17c05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:07:28.679162Z",
     "start_time": "2024-06-16T12:07:26.019061Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "openai_api = OpenAI()\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# Test tagging the education dictionary\n",
    "result = evaluate_result(\n",
    "    os.getenv('Test_FILE_PATH'),\n",
    "    sys_prompt, \n",
    "    openai_api)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65045b4c77f26a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:57:30.284661Z",
     "start_time": "2024-06-16T12:24:21.503742Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "openai_api = OpenAI()\n",
    "\n",
    "# Directory containing all files to process\n",
    "results_directory = os.getenv('TAGS_FOLDER_PATH') + 'outputs_json/'\n",
    "\n",
    "# Read the system prompt\n",
    "with open('prompt.txt', 'r') as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "# List to hold all the results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all files in the results directory\n",
    "for filename in os.listdir(results_directory):\n",
    "    if filename.endswith(\".json\"):  # Adjust the file type if necessary\n",
    "        file_path = os.path.join(results_directory, filename)\n",
    "        # Evaluate the result for each file\n",
    "        result = evaluate_result(file_path, sys_prompt, openai_api)\n",
    "        # Append the result to the list\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d9c05a292a175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:57:34.636209Z",
     "start_time": "2024-06-16T12:57:34.254480Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Print the number of results\n",
    "print(len(all_results))\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_path = f'evaluation_{len(all_results)}_gpt_4o.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(all_results, json_file, indent=4)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "eval_df = pd.DataFrame(all_results)\n",
    "eval_df.to_csv(f'evaluation_{len(all_results)}_gpt_4o.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc8d7b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "# Specify the columns you want to keep\n",
    "columns_to_keep = [\n",
    "    \"valid_json\",\n",
    "    \"accuracy_rating\",\n",
    "    \"assumption_rating\",\n",
    "    \"justification_rating\",\n",
    "    \"missing_info_rating\",\n",
    "    \"over_generalization_rating\",\n",
    "    \"confidence_score\",\n",
    "    \"comments\"\n",
    "]\n",
    "\n",
    "# Load the evaluation results from the JSON file and filter to only include specified columns\n",
    "data = pd.read_json(os.getenv('RESULT_JSON_PATH'))[columns_to_keep] \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983e1f1",
   "metadata": {},
   "source": [
    "#### Scores\n",
    "*Scale of 1 to 10, where 1 indicates poor performance and 10 indicates excellent performance.*\n",
    "\n",
    "**JSON Validity**<br>\n",
    "Check if the generated output follows a valid JSON structure.\n",
    "\n",
    "**Accuracy**<br>\n",
    "Evaluate how accurate and relevant the generated tags and skills are based on the provided experience. Consider factors like job title, company, and description.\n",
    "\n",
    "**Assumptions**<br>\n",
    "Assess whether the model made reasonable assumptions or if it generated tags and skills that are far-fetched or not well-justified given the available information.\n",
    "\n",
    "**Missing Information**<br>\n",
    "Check if the model missed any important tags or skills that should have been included based on the experience.\n",
    "\n",
    "**Over-generalization**<br>\n",
    "Evaluate if the model generated overly generic or broad tags and skills that might not be directly relevant to the specific experience.\n",
    "\n",
    "**Confidence**<br>\n",
    "Provide an overall confidence score for the generated output, considering the accuracy, relevance, and justification of the tags and skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Calculate the averages for the ratings\n",
    "average_ratings = data[columns_to_keep[1:7]].mean() * 10\n",
    "\n",
    "# Calculate the percentage of valid_json being True\n",
    "valid_json_percentage = data['valid_json'].mean() * 100\n",
    "\n",
    "# Add the valid_json percentage to the average ratings\n",
    "average_ratings['valid_json_percentage'] = valid_json_percentage\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_ratings.plot(kind='bar')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Average ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to plot\n",
    "columns_to_plot = [\n",
    "    \"accuracy_rating\",\n",
    "    \"assumption_rating\",\n",
    "    \"justification_rating\",\n",
    "    \"missing_info_rating\",\n",
    "    \"over_generalization_rating\",\n",
    "    \"confidence_score\"\n",
    "]\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "data[columns_to_plot].boxplot()\n",
    "plt.title('Ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Convert DataFrame to long format for Seaborn\n",
    "melted_data = data.melt(value_vars=columns_to_plot, var_name='Rating Type', value_name='Rating')\n",
    "\n",
    "# Plotting the violin plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x='Rating Type', y='Rating', data=melted_data, inner='box', density_norm='width')\n",
    "plt.title('Ratings')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.ylim(1, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(\"Test\")",
   "id": "73b436e132323cfe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
